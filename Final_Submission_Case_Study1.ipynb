{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Final Submission"
      ],
      "metadata": {
        "id": "qHvNhMttAJ10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Final.ipynb file should contain the following :\n",
        "\n",
        "    It should have two functions.\n",
        "    Function-1\n",
        "        Should include entire pipeline, from data preprocessing to making final predictions.\n",
        "        It should take in raw data as input.\n",
        "        It should return predictions for your input. Here the input can be a single point or a set of points.\n",
        "        def final_fun_1(X):\n",
        "        .....\n",
        "        .....\n",
        "        ..... # you will use the best model that you found out with your experiments\n",
        "        return predictions made on X ( Raw Data)\n",
        "    Function-2\n",
        "        Should include entire pipeline, from data preprocessing to making final predictions.\n",
        "        It should take in raw data as input along with its target values.\n",
        "        It should return the metric value that you are judging your models on.\n",
        "        def final_fun_2(X,Y):\n",
        "        .....\n",
        "        .....\n",
        "        ..... # you will use the best model that you found out with your experiments\n",
        "        return final_metric computed on X ( Raw Data) and Y (target variable)"
      ],
      "metadata": {
        "id": "RLLEmzVZ__rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kgJsZE4wC5V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT4zc3exBwRm",
        "outputId": "77f54014-19e7-4fff-88f2-0490a1035842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing all the required libraries"
      ],
      "metadata": {
        "id": "IVR2-6CwC_cA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKo7SThkBwVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import scipy\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as plot\n",
        "import seaborn as sn\n",
        "import joblib\n",
        "import warnings\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm_notebook, tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "import gc\n",
        "from scipy.sparse import hstack\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FILJMHsBwZZ",
        "outputId": "6974bf3d-8cc4-464d-8d4c-3ab0b594dbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function -- 1"
      ],
      "metadata": {
        "id": "sFSlEOn_DkXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function1(X):    \n",
        "    \"\"\"\n",
        "    Description -> Accepts input X as raw data\n",
        "    The function returns predicted price\n",
        "    \"\"\"\n",
        "    X.fillna('', inplace=True)\n",
        "    X['item_description']  = X['item_description'].str.replace('^no description yet$', '', regex=True)\n",
        "    \n",
        "    # Reference: https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussion/50256\n",
        "    # https://github.com/divyanshjain19/Mercari_Price_Suggestion/blob/master/2_Preprocessing_and_featurizations.ipynb\n",
        "    X['name'] = X['name'] + \" \" + X['brand_name']\n",
        "    X['text'] = X['item_description'] + \" \" + X['name'] + \" \" + X['category_name']\n",
        "\n",
        "\n",
        "    df_train = joblib.load('/content/gdrive/MyDrive/binary_files/df_train_26Jan23_v1.joblib')\n",
        "\n",
        "    # Ref: AAIC Notebook for Donors' Choose\n",
        "    def decontracted(phrase):\n",
        "        '''\n",
        "        Description -> Replaces the short form words to their decontracted form such as won't to will not, \n",
        "                      this is done to make the text data uniform using regex commands.\n",
        "        '''\n",
        "        phrase = re.sub(r\"aren\\'t\", \"are not\", phrase)\n",
        "        phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
        "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "        phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
        "        phrase = re.sub(r\"won\\'t\", \"would not\", phrase)\n",
        "        phrase = re.sub(r\"wouldn\\'t\", \"would not\", phrase)\n",
        "        phrase = re.sub(r\"haven\\'t\", \"have not\", phrase)\n",
        "        phrase = re.sub(r\"shouldn\\'t\", \"should not\", phrase)\n",
        "        phrase = re.sub(r\"doesn\\'t\", \"does not\", phrase)\n",
        "        phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n",
        "        phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
        "        phrase = re.sub(r\"mustn\\'t\", \"must not\", phrase)\n",
        "        phrase = re.sub(r\"needn\\'t\", \"need not\", phrase)\n",
        "        \n",
        "        return phrase\n",
        "\n",
        "        X['name'] = X['name'].apply(lambda x : decontracted(x))\n",
        "        X['text'] = X['text'].apply(lambda x : decontracted(x))\n",
        "\n",
        "\n",
        "    #Reference: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
        "    #Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item\n",
        "    def perform_lemma(sent,all_stopwords):\n",
        "        '''\n",
        "        Description -> Applying wordnet lemmatizer on the input sentence and returning the nearest base word/sentence.\n",
        "        '''    \n",
        "        sent_list = sent.split()\n",
        "        lem = WordNetLemmatizer()\n",
        "        text = [lem.lemmatize(word) for word in sent_list if word not in all_stopwords]\n",
        "        sent = \" \".join(text)\n",
        "        return sent\n",
        "\n",
        "    regex_special_chars = re.compile('[^A-Za-z0-9.]+')\n",
        "    regex_decimal_digits = re.compile('(?<!\\d)\\.(?!\\d)')\n",
        "    regex_white_space = re.compile(r'\\s+')\n",
        "\n",
        "    # Since considering positive and negative emotion of buyer, so considering negative words also\n",
        "    # Reference : https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
        "    # all_stopwords = stopwords.words('english')\n",
        "    # all_stopwords.remove('not')\n",
        "    # all_stopwords.remove('nor')\n",
        "    # all_stopwords.remove('no')\n",
        "    #optimized way\n",
        "    all_stopwords = set(stopwords.words(\"english\")) - {\"no\", \"nor\", \"not\"} \n",
        "\n",
        "\n",
        "    def process_text_data(sent):\n",
        "        '''\n",
        "        Description -> Calling the demojify and lemmatization functions one by one with data as the input\n",
        "                      and returning the final preprocessed data.\n",
        "        '''\n",
        "        \n",
        "        #Removing new line, carriage return, double quotes\n",
        "        sent = sent.replace('\\\\r', ' ')\n",
        "        #sent = sent.replace('\\\\\"', ' ')\n",
        "        sent = sent.replace('\\\\n', ' ')\n",
        "        #remove enojis  https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
        "        #sent = deEmojify(sent)\n",
        "\n",
        "        #removing all special charecter except mentioned in regex strings    \n",
        "        sent = regex_special_chars.sub(' ', sent)\n",
        "\n",
        "        #removing all digit mentioned in regex strings\n",
        "        sent = regex_decimal_digits.sub(' ', sent)\n",
        "        \n",
        "        # removing all white spaces  https://bobbyhadz.com/blog/python-remove-whitespace-regex      \n",
        "        sent = regex_white_space.sub(' ', sent)\n",
        "        #sent = re.sub(r'^\\s+', '', sent, flags=re.MULTILINE)\n",
        "\n",
        "        #removing end space and converting to lower\n",
        "        sent = sent.strip().lower()\n",
        "\n",
        "        # only take the words which are not stop words\n",
        "        sent = perform_lemma(sent, all_stopwords)    \n",
        "        return sent\n",
        "\n",
        "    X['name'] = X['name'].apply(lambda x : process_text_data(x))\n",
        "    X['text'] = X['text'].apply(lambda x : process_text_data(x))\n",
        "\n",
        "    def text_encoder_testdata(train_data,test_data,type,params):\n",
        "        '''\n",
        "        Description -> Encoding different types of input text data according to its requirements using Countvectorizer\n",
        "                      & Tfidfvectorizer and returning the transformed data as output\n",
        "        '''\n",
        "        if(type == \"BOW\"):\n",
        "            vectorizer = CountVectorizer(ngram_range = params[0],min_df = params[1],max_df = params[2],max_features = params[3])\n",
        "        elif(type == \"TFIDF\"):\n",
        "            N_GRAMS =params\n",
        "            vectorizer = vectorizer = TfidfVectorizer(max_features = 100000,\n",
        "                                    ngram_range = (1, N_GRAMS),\n",
        "                                    strip_accents = 'unicode',\n",
        "                                    analyzer = 'word',\n",
        "                                    token_pattern = r'\\w+')\n",
        "        elif(type==\"CNTVECT\"):\n",
        "            vectorizer = CountVectorizer(vocabulary=params, lowercase=False, binary=True)\n",
        "\n",
        "        #Vectorize on train data and transform on test data\n",
        "        vectorizer.fit(train_data)    \n",
        "        test_transform = vectorizer.transform(test_data)\n",
        "\n",
        "        if (type == \"BOW\"):\n",
        "            return test_transform,  ''\n",
        "        elif (type == \"CNTVECT\"):\n",
        "            return test_transform, ''\n",
        "        elif (type == \"TFIDF\"):\n",
        "            feat_names = vectorizer.get_feature_names_out()\n",
        "            del vectorizer\n",
        "            gc.collect()\n",
        "            return test_transform, feat_names\n",
        "\n",
        "    X_test_name,_ = text_encoder_testdata(df_train['name'],X['name'],\"TFIDF\", 1)\n",
        "    X_test_text,_ = text_encoder_testdata(df_train['text'],X['text'],\"TFIDF\", 2)\n",
        "\n",
        "    #X_test_name.shape,X_test_text.shape\n",
        "\n",
        "    def one_hot_encoder_testdata(train_data,test_data):\n",
        "        ohe_encoder = OneHotEncoder()\n",
        "        #Vectorize on train data and transform on test data\n",
        "        ohe_encoder.fit(train_data)\n",
        "        test_ohe = ohe_encoder.transform(test_data)\n",
        "        return test_ohe\n",
        "      \n",
        "    X_test_shipping = one_hot_encoder_testdata(np.reshape(df_train['shipping'].values, (-1, 1)),np.reshape(X['shipping'].values, (-1, 1)))\n",
        "    X_test_item_condition = one_hot_encoder_testdata(np.reshape(df_train['item_condition_id'].values, (-1, 1)),np.reshape(X['item_condition_id'].values, (-1, 1)))\n",
        "\n",
        "    testframe = hstack((X_test_name,\n",
        "                  X_test_text,\n",
        "                  X_test_shipping,\n",
        "                  X_test_item_condition)).tocsr().astype('float32')\n",
        "\n",
        "    #testframe.shape\n",
        "    \n",
        "    loaded_model1 = tf.keras.models.load_model('/content/gdrive/MyDrive/binary_files/model1.hdf5')\n",
        "    loaded_model2 = tf.keras.models.load_model('/content/gdrive/MyDrive/binary_files/model2.hdf5')\n",
        "\n",
        "    y_pred_test       = loaded_model1.predict(testframe)[:, 0]\n",
        "    y_pred_test_model1 = np.expm1(y_pred_test.reshape(-1, 1))[:, 0]\n",
        "\n",
        "    y_pred_test       = loaded_model2.predict(testframe)[:, 0]\n",
        "    y_pred_test_model2 = np.expm1(y_pred_test.reshape(-1, 1))[:, 0]\n",
        "\n",
        "    # from training \n",
        "    wmin =0.405\n",
        "    final_predictions_test = wmin*y_pred_test_model1 + (1-wmin)*y_pred_test_model2\n",
        "    print(final_predictions_test)\n",
        "\n",
        "    return final_predictions_test\n"
      ],
      "metadata": {
        "id": "2GDo7vwsebmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function -- 2\n"
      ],
      "metadata": {
        "id": "TjQ36IDODkfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function2(X,y_true):\n",
        "    \"\"\"\n",
        "    Description -> Accepts input X and y_true as raw data and target values\n",
        "    The function returns metric value\n",
        "    \"\"\"\n",
        "    y_pred=function1(X)\n",
        "    return np.sqrt(mean_squared_log_error(y_true,y_pred))\n"
      ],
      "metadata": {
        "id": "yBjH0xp71iku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on Large Set"
      ],
      "metadata": {
        "id": "bIDpXU6Oii2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test= pd.read_csv('/content/gdrive/MyDrive/train.tsv', sep='\\t')\n",
        "predicted_price = function1(df_test)"
      ],
      "metadata": {
        "id": "pxmqi1-Khu5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2586848c-a1d3-419e-87a5-7878cca1ac76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46330/46330 [==============================] - 106s 2ms/step\n",
            "46330/46330 [==============================] - 251s 5ms/step\n",
            "[ 9.894726 56.904846 10.494579 ... 15.027779 14.315481 25.384705]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[1:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NE7CkcJhMXH",
        "outputId": "7c540c6e-e9df-4f06-970d-ed15ae4d4375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([56.904846, 10.494579, 29.515343, 23.389828, 69.261856, 40.889202,\n",
              "        7.286416, 19.687225, 11.361588], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "price_original = df_test['price']\n",
        "print(\"RMSLE is = \", function2(df_test,price_original))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z6mB0tP2Q2B",
        "outputId": "5be9df5f-ce36-46d0-e528-60bcb3434561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46330/46330 [==============================] - 103s 2ms/step\n",
            "46330/46330 [==============================] - 268s 6ms/step\n",
            "[ 9.894726 54.59494   9.135599 ... 15.027779 14.315481 25.384705]\n",
            "RMSLE is =  0.28858137497814784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on Small Set"
      ],
      "metadata": {
        "id": "7bpdBBQNiYtm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lk8HYWGohZnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test= pd.read_csv('/content/gdrive/MyDrive/train.tsv', sep='\\t')[20:30]\n",
        "predicted_price = function1(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9-FedBnhZxA",
        "outputId": "4118ae09-fa0e-4353-91f8-b33e4bcc950b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "[ 13.916389   22.321388  301.16        7.9210873  10.539222   75.43791\n",
            "  18.355772   28.235123   12.587719   15.591455 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "price_original = df_test['price']\n",
        "print(\"RMSLE is = \", function2(df_test,price_original))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uupUom5Bhebe",
        "outputId": "6823b4cb-fbac-4548-95cf-0f81b60faa6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 406ms/step\n",
            "1/1 [==============================] - 1s 572ms/step\n",
            "[ 13.930695   23.001     277.09552     7.9210873  10.628723   64.76397\n",
            "  18.21217    26.504126   11.667847   14.220484 ]\n",
            "RMSLE is =  0.28542474902077025\n"
          ]
        }
      ]
    }
  ]
}